@inproceedings{affeldt2020EnsembleBlockCoclustering,
  title = {Ensemble Block Co-Clustering: {{A}} Unified Framework for Text Data},
  shorttitle = {Ensemble {{Block Co-clustering}}},
  booktitle = {International {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Affeldt, S{\'e}verine and Labiod, Lazhar and Nadif, Mohamed},
  year = {2020},
  month = oct,
  pages = {5--14},
  publisher = {{ACM}},
  address = {{Virtual Event Ireland, France}},
  doi = {10.1145/3340531.3412058},
  urldate = {2023-09-20},
  abstract = {In this paper, we propose a unified framework for Ensemble Block Co-clustering (EBCO), which aims to fuse multiple basic co-clusterings into a consensus structured affinity matrix. Each co-clustering to be fused is obtained by applying a co-clustering method on the same document-term dataset. This fusion process reinforces the individual quality of the multiple basic data co-clusterings within a single consensus matrix. Besides, the proposed framework enables a completely unsupervised co-clustering where the number of co-clusters is automatically inferred based on the non trivial generalized modularity. We first define an explicit objective function which allows the joint learning of the basic co-clusterings aggregation and the consensus block co-clustering. Then, we show that EBCO generalizes the one side ensemble clustering to an ensemble block co-clustering context. We also establish theoretical equivalence to spectral co-clustering and weighted double spherical k-means clustering for textual data. Experimental results on various real-world document-term datasets demonstrate that EBCO is an efficient competitor to some state-of-the-art ensemble and co-clustering methods.}
}

@article{zheng2017SupervisedAdaptiveIncremental,
  title = {Supervised {{Adaptive Incremental Clustering}} for Data Stream of Chunks},
  author = {Zheng, Laiwen and Huo, Hong and Guo, Yiyou and Fang, Tao},
  year = {2017},
  month = jan,
  journal = {Neurocomputing},
  volume = {219},
  pages = {502--517},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2016.09.054},
  urldate = {2023-09-21},
  abstract = {Many supervised clustering algorithms have been developed to find the optimal clusters for static datasets by presetting some parameters, but they are seldom suitable for dynamic datasets, such as the data stream of chunks. To find the optimal clusters of the data stream of chunks, a novel Supervised Adaptive Incremental Clustering (SAIC) algorithm is proposed. SAIC can cluster dynamic datasets of arbitrary shapes and sizes automatically. It includes learning and post-processing phases. In the learning phase, each cluster updates adaptively according to its learning rate that is calculated from its counter value. All data points are shuffled at each iteration in order to make SAIC insensitive to the input order of data points. In the post-processing phase, the outliers or boundary points are eliminated according to the counter value of each cluster and the number of iterations. Four synthetic datasets and fourteen UCI datasets are used to evaluate the performance of SAIC, respectively. The experiments on UCI datasets show that SAIC reaches to or outperforms some other supervised clustering algorithms and several unsupervised incremental clustering algorithms. In addition, three data stream of chunks are used to evaluate SAIC from different aspects, which shows SAIC has the scalability and incremental learning ability for the clustering of data streams of chunks.}
}
